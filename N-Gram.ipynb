{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Model and Its Use in the Code\n",
    "\n",
    "## What is an N-gram?\n",
    "\n",
    "- **Unigram**: An n-gram where `n=1`. It consists of single words.\n",
    "  - Example: `\"I love NLP\"` → `[\"I\", \"love\", \"NLP\"]`\n",
    "  \n",
    "- **Bigram**: An n-gram where `n=2`. It consists of pairs of consecutive words.\n",
    "  - Example: `\"I love NLP\"` → `[\"I love\", \"love NLP\"]`\n",
    "  \n",
    "- **Trigram**: An n-gram where `n=3`. It consists of triplets of consecutive words.\n",
    "  - Example: `\"I love NLP\"` → `[\"I love NLP\"]`\n",
    "\n",
    "## Purpose of N-grams in NLP\n",
    "\n",
    "1. **Language Modeling**: N-grams are used to model the likelihood of a word given the previous words in a sequence.\n",
    "2. **Text Generation**: By training on a corpus of text, an n-gram model can generate text by predicting the next word in a sequence based on the previous `n-1` words.\n",
    "3. **Perplexity**: A measure used to evaluate how well a language model predicts a sample. Lower perplexity indicates a better model.\n",
    "\n",
    "## What N-grams are Doing in This Code\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The `LanguageModel_NGram` class builds an n-gram model with options for different values of `n`.\n",
    "   - `is_laplace_smoothing` controls whether Laplace smoothing is applied to handle zero probabilities for unseen n-grams.\n",
    "\n",
    "2. **Data Loading and Cleaning**:\n",
    "   - Loads song lyrics from a dataset and preprocesses the text by tokenizing, removing punctuation and stopwords, and converting text to lowercase.\n",
    "\n",
    "3. **N-gram Tokenization**:\n",
    "   - The `ngram_tokenization` method generates n-grams from preprocessed sentences.\n",
    "   - Example: If `n=3`, it creates trigrams from the words in each sentence.\n",
    "\n",
    "4. **Counting Frequencies**:\n",
    "   - Counts the frequency of each n-gram and stores this in a dictionary (`ngram_token_count_dict`).\n",
    "   - Counts the unique occurrences of `(n-1)`-grams to calculate the probabilities of each n-gram.\n",
    "\n",
    "5. **Training the Model**:\n",
    "   - The `train` method uses frequency counts to compute the probability of each n-gram.\n",
    "   - Laplace smoothing adjusts probabilities for unseen n-grams if enabled.\n",
    "\n",
    "6. **Scoring and Perplexity**:\n",
    "   - The `score` method calculates the probability of a sentence by multiplying the probabilities of the n-grams in the sentence.\n",
    "   - The `perplexity` method evaluates how well the model predicts a test sentence, providing a measure of the model's quality.\n",
    "\n",
    "7. **Text Generation**:\n",
    "   - The `generate_text` method uses the trained n-gram model to generate text by selecting the most probable next word until a specified length or an end token (`</s>`) is reached.\n",
    "\n",
    "8. **Evaluation**:\n",
    "   - The `evaluate_model` method assesses the model's performance by calculating perplexity on training and validation data.\n",
    "\n",
    "9. **ROUGE Score Calculation**:\n",
    "   - The `calculate_rouge_score` function computes ROUGE scores to evaluate the quality of generated text by comparing it with reference texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel_NGram:\n",
    "    UNK = \"<UNK>\"  # Token for unknown words\n",
    "    SENT_BEGIN = \"<s>\"  # Token for the beginning of a sentence\n",
    "    SENT_END = \"</s>\"  # Token for the end of a sentence\n",
    "\n",
    "    def __init__(self, n_gram=3, is_laplace_smoothing=1):\n",
    "        self.n_gram = n_gram  # Set the n-gram size\n",
    "        self.is_laplace = is_laplace_smoothing  # Set Laplace smoothing flag\n",
    "        \n",
    "        # Initialize data structures\n",
    "        self.data = pd.DataFrame()  # Raw dataset\n",
    "        self.lyric_dic = {}  # Dictionary to store lyrics by song title\n",
    "        self.corpus = []  # List to store all lyrics combined\n",
    "        self.lyric_dic_updated = {}  # Processed lyrics with special tokens\n",
    "        self.lyric_dic_processed = {}  # Further processed lyrics for n-gram analysis\n",
    "        self.token_count_dic = {}  # Token frequency dictionary\n",
    "        self.ngram_token_count_dict = {}  # N-gram token frequency dictionary\n",
    "        self.total_tokens = 0  # Total number of tokens in the corpus\n",
    "        self.total_vocab = 0  # Total number of unique tokens in the corpus\n",
    "        self.unk_words = []  # List to hold rare words considered as unknown tokens\n",
    "        \n",
    "        self.prob_dic = {}  # Dictionary to hold probabilities of n-grams\n",
    "        self.ngram_tokenized_words = []  # List to store tokenized n-grams\n",
    "        self.unique_tokens_to_count_previous_dic = {}  # Dictionary for unique token counts\n",
    "\n",
    "    def load_dataset(self, link_to_dataset):\n",
    "        self.data = pd.read_csv(link_to_dataset, encoding='utf-8')  # Load the dataset into a DataFrame\n",
    "        print(self.data.head(20))  # Print the first 20 rows of the dataset for inspection\n",
    "        \n",
    "    def dataset_cleaning(self):\n",
    "        lyric_dic = {}  # Initialize a dictionary to store lyrics by song\n",
    "        corpus = []  # List to store all lyrics\n",
    "        \n",
    "        data = self.data  # Reference the dataset\n",
    "        prev = data[\"Song Title\"].iloc[0]  # Initialize previous song title\n",
    "        temp = []  # Temporary list to accumulate lyrics for the same song\n",
    "\n",
    "        # Group lyrics by song title\n",
    "        for index, rows in data.iterrows():  # Iterate through each row in the DataFrame\n",
    "            lyrics = rows[\"Lyrics\"]  # Extract lyrics from the current row\n",
    "            if pd.isna(lyrics):  # Skip rows with missing lyrics\n",
    "                continue\n",
    "            corpus.append(lyrics)  # Add lyrics to the corpus\n",
    "            if prev == rows[\"Song Title\"]:  # Check if current song title matches previous\n",
    "                temp.append(lyrics)  # Accumulate lyrics for the same song\n",
    "            else:\n",
    "                lyric_dic[prev] = temp  # Save accumulated lyrics for the previous song\n",
    "                prev = rows[\"Song Title\"]  # Update previous song title\n",
    "                temp = []  # Reset temporary list\n",
    "                temp.append(lyrics)  # Start accumulating lyrics for the new song\n",
    "        lyric_dic[prev] = temp  # Add the last accumulated lyrics\n",
    "\n",
    "        self.lyric_dic = lyric_dic  # Store the lyrics dictionary\n",
    "        self.corpus = corpus  # Store the combined lyrics corpus\n",
    "\n",
    "        # Preprocess lyrics for each song\n",
    "        lyric_dic_updated = {}  # Dictionary to hold preprocessed lyrics\n",
    "        for song in lyric_dic:  # Iterate through each song\n",
    "            t_songs = []  # List to hold processed lyrics for a song\n",
    "            for lyrics in lyric_dic[song]:  # Process each set of lyrics\n",
    "                t_songs.append(self.preprocess_text(lyrics))  # Preprocess and add to list\n",
    "            lyric_dic_updated[song] = t_songs  # Save processed lyrics\n",
    "\n",
    "        self.lyric_dic_updated = lyric_dic_updated  # Store the updated lyrics dictionary\n",
    "\n",
    "        # Tokenize and preprocess lyrics with special tokens\n",
    "        lyric_dic_processed = {}  # Dictionary to hold further processed lyrics\n",
    "        for song_name in lyric_dic_updated:  # Iterate through each song\n",
    "            temp_array = [\"<s>\"] * (self.n_gram - 1)  # Add start tokens\n",
    "\n",
    "            for sentence in lyric_dic_updated[song_name]:  # Process each sentence\n",
    "                temp = sentence.split()  # Split sentence into words\n",
    "                temp[-1] = temp[-1] + \"/n\"  # Append new line token to the last word\n",
    "                temp_array.extend(temp)  # Add words to the array\n",
    "\n",
    "            temp_array.extend([\"</s>\"] * (self.n_gram - 1))  # Add end tokens\n",
    "            lyric_dic_processed[song_name] = temp_array  # Save processed lyrics\n",
    "        \n",
    "        self.lyric_dic_processed = lyric_dic_processed  # Store the processed lyrics dictionary\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        if isinstance(text, str):  # Ensure the text is a string\n",
    "            text = text.lower()  # Convert text to lowercase\n",
    "            text = \"\".join([i for i in text if i not in string.punctuation])  # Remove punctuation\n",
    "            text = \" \".join([i for i in text.split() if i not in stopwords.words('english')])  # Remove stopwords\n",
    "        else:\n",
    "            text = \"\"\n",
    "        return text\n",
    "    \n",
    "    def ngram_tokenization(self, sentences):\n",
    "        tokens = []  # List to hold n-grams\n",
    "        for sentence in sentences:  # Iterate through each sentence\n",
    "            sen = sentence.split()  # Split sentence into words\n",
    "            for t in range(len(sen) - self.n_gram + 1):  # Generate n-grams\n",
    "                tokens.append(tuple(sen[t:t + self.n_gram]))  # Add n-gram to list\n",
    "        return tokens\n",
    "    \n",
    "    def count_all_token_freq(self, tokenized_words):\n",
    "        d = {}  # Dictionary to hold token frequencies\n",
    "        for token in tokenized_words:  # Iterate through each token\n",
    "            temp_tuple = tuple(token)  # Convert token to tuple\n",
    "            if temp_tuple not in d:  # Check if token is already in dictionary\n",
    "                d[temp_tuple] = 1  # Initialize frequency count\n",
    "            else:\n",
    "                d[temp_tuple] += 1  # Increment frequency count\n",
    "        return d\n",
    "    \n",
    "    def count_unique_tokens(self, sentences_from_file):\n",
    "        word_d = {}  # Dictionary to hold unique token frequencies\n",
    "        for sen in sentences_from_file:  # Iterate through each sentence\n",
    "            sen = sen.split()  # Split sentence into words\n",
    "            for t in range(len(sen) - self.n_gram + 1):  # Generate n-grams\n",
    "                if self.n_gram == 1:\n",
    "                    token = tuple(sen[t:t + self.n_gram])  # Create unigram\n",
    "                else:\n",
    "                    token = tuple(sen[t:t + self.n_gram - 1])  # Create n-gram of size n-1\n",
    "                \n",
    "                if token not in word_d:  # Check if token is already in dictionary\n",
    "                    word_d[token] = 1  # Initialize frequency count\n",
    "                else:\n",
    "                    word_d[token] += 1  # Increment frequency count\n",
    "        return word_d\n",
    "\n",
    "    def dataset_preprocess(self):\n",
    "        token_count_dic = {}  # Dictionary to hold token counts\n",
    "        count = 0  # Initialize total token count\n",
    "        for song_name in self.lyric_dic_updated:  # Iterate through each song\n",
    "            for sentence in self.lyric_dic_updated[song_name]:  # Process each sentence\n",
    "                for word in sentence.split():  # Split sentence into words\n",
    "                    if word not in token_count_dic:  # Check if word is already in dictionary\n",
    "                        token_count_dic[word] = 1  # Initialize frequency count\n",
    "                    else:\n",
    "                        token_count_dic[word] += 1  # Increment frequency count\n",
    "                    count += 1  # Increment total token count\n",
    "        self.total_tokens = count  # Set total token count\n",
    "        self.token_count_dic = token_count_dic  # Store token count dictionary\n",
    "\n",
    "    def display_wordcloud(self):\n",
    "        text = \"\".join(self.corpus)  # Combine all lyrics into a single text\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)  # Generate word cloud\n",
    "        plt.figure(figsize=(10, 5))  # Set figure size for display\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')  # Display the word cloud\n",
    "        plt.axis(\"off\")  # Hide axis\n",
    "        plt.title(\"Word Cloud of Song Lyrics\")  # Set title\n",
    "        plt.show()  # Show the plot\n",
    "        \n",
    "        \n",
    "    def display_stats_before(self):\n",
    "        print(\"Dataset Information before preprocessing\")\n",
    "        corpus = self.corpus  # Reference the corpus\n",
    "        \n",
    "        num_of_songs = len(self.lyric_dic)  # Get number of songs\n",
    "        print(\"Number of songs =\", num_of_songs)\n",
    "\n",
    "        vocab = set()  # Set to hold unique words\n",
    "        token_size = 0  # Initialize token size count\n",
    "        word_count = {}  # Dictionary to hold word frequencies\n",
    "        \n",
    "        for sentence in corpus:  # Iterate through each sentence\n",
    "            tokens = sentence.split()  # Split sentence into words\n",
    "            vocab.update(tokens)  # Update vocabulary with new words\n",
    "            token_size += len(tokens)  # Update token size count\n",
    "            for word in tokens:  # Count word frequencies\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "        \n",
    "        print(\"Vocab size =\", len(vocab))  # Print vocabulary size\n",
    "        print(\"Token size =\", token_size)  # Print token size\n",
    "        \n",
    "        word_count_cutoff = {k: v for k, v in word_count.items() if v > 50}  # Filter words with frequency > 50\n",
    "        plt.figure(figsize=(12, 8))  # Set figure size for display\n",
    "        plt.bar(word_count_cutoff.keys(), word_count_cutoff.values())  # Create bar plot\n",
    "        plt.xticks(rotation=-45)  # Rotate x-axis labels for better visibility\n",
    "        plt.title('Word Frequency Cutoff')  # Set title\n",
    "        plt.show()  # Show the plot\n",
    "        \n",
    "        sample_items = list(self.lyric_dic.items())[:1]  # Get a sample song\n",
    "        print(\"Sample song lyrics:\", sample_items)  # Print the sample song\n",
    "\n",
    "    def train(self):\n",
    "        self.total_unique_tokens = len(self.token_count_dic)  # Set total unique token count\n",
    "        \n",
    "        for token in self.token_count_dic:  # Iterate through each token\n",
    "            if self.token_count_dic[token] == 1:  # Check if token occurs only once\n",
    "                self.unk_words.append(token)  # Add to list of unknown words\n",
    "        \n",
    "        all_sentences = []  # List to hold tokenized sentences\n",
    "        for song_name in self.lyric_dic_processed:  # Iterate through each song\n",
    "            t = \" \".join(self.lyric_dic_processed[song_name])  # Combine tokens into a single string\n",
    "            all_sentences.append(t)  # Add to list of all sentences\n",
    "        \n",
    "        self.ngram_tokenized_words = self.ngram_tokenization(all_sentences)  # Generate n-grams\n",
    "        self.ngram_token_count_dict = self.count_all_token_freq(self.ngram_tokenized_words)  # Count n-gram frequencies\n",
    "        self.unique_tokens_to_count_previous_dic = self.count_unique_tokens(all_sentences)  # Count unique n-grams\n",
    "        \n",
    "        self.prob_dic = {}  # Dictionary to hold n-gram probabilities\n",
    "        for token in self.ngram_token_count_dict:  # Iterate through each n-gram\n",
    "            word = token[:-1]  # Get the prefix of the n-gram\n",
    "            if self.is_laplace:\n",
    "                self.prob_dic[token] = (self.ngram_token_count_dict[token] + 1) / (\n",
    "                            self.unique_tokens_to_count_previous_dic[word] + self.total_unique_tokens)\n",
    "            else:\n",
    "                self.prob_dic[token] = self.ngram_token_count_dict[token] / self.unique_tokens_to_count_previous_dic[word]\n",
    "        \n",
    "        print(\"Sample probabilities:\", list(self.prob_dic.items())[:10])  # Print sample probabilities\n",
    "\n",
    "    def score(self, sentence):\n",
    "        score = 1  # Initialize score\n",
    "        sen = sentence.split()  # Split sentence into words\n",
    "        token = [\"<s>\"] * (self.n_gram - 1)  # Initialize start tokens\n",
    "        \n",
    "        for t in range(len(sen)):  # Iterate through each word in the sentence\n",
    "            token.append(sen[t])  # Add word to token list\n",
    "            token_temp = tuple(token[t:t + self.n_gram])  # Create n-gram\n",
    "            if token_temp in self.prob_dic:  # Check if n-gram is in probability dictionary\n",
    "                score *= self.prob_dic[token_temp]  # Multiply score by n-gram probability\n",
    "            else:\n",
    "                score *= 1 / (self.total_unique_tokens + 1)  # Use unknown token probability\n",
    "        return score  # Return final score\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        sen = sentence.split()  # Split sentence into words\n",
    "        n = len(sen)  # Get number of words in the sentence\n",
    "        log_prob_sum = 0  # Initialize sum of log probabilities\n",
    "        \n",
    "        token = [\"<s>\"] * (self.n_gram - 1)  # Initialize start tokens\n",
    "        for t in range(n):  # Iterate through each word in the sentence\n",
    "            token.append(sen[t])  # Add word to token list\n",
    "            token_temp = tuple(token[t:t + self.n_gram])  # Create n-gram\n",
    "            if token_temp in self.prob_dic:  # Check if n-gram is in probability dictionary\n",
    "                log_prob_sum += math.log(self.prob_dic[token_temp])  # Add log probability\n",
    "            else:\n",
    "                log_prob_sum += math.log(1 / (self.total_unique_tokens + 1))  # Use log of unknown token probability\n",
    "        \n",
    "        perplexity = math.exp(-log_prob_sum / n)  # Calculate perplexity\n",
    "        return perplexity  # Return perplexity\n",
    "    \n",
    "    def generate_text(self, max_length=100):\n",
    "        \"\"\"\n",
    "        Generate text using the trained n-gram model.\n",
    "\n",
    "        :param max_length: Maximum length of the generated text\n",
    "        :return: Generated text\n",
    "        \"\"\"\n",
    "        current_tokens = [\"<s>\"] * (self.n_gram - 1)  # Start with sentence start tokens\n",
    "        generated_text = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            current_ngram = tuple(current_tokens[-(self.n_gram - 1):])  # Get the current n-gram prefix\n",
    "            possible_next_tokens = [token for token in self.prob_dic if token[:-1] == current_ngram]  # Find possible next tokens\n",
    "\n",
    "            if not possible_next_tokens:\n",
    "                break  # No possible next token, end generation\n",
    "\n",
    "            next_token = max(possible_next_tokens, key=lambda token: self.prob_dic[token])[-1]  # Select the most probable next token\n",
    "            if next_token == \"</s>\":\n",
    "                break  # End token reached, stop generation\n",
    "\n",
    "            generated_text.append(next_token)  # Add the next token to the generated text\n",
    "            current_tokens.append(next_token)  # Update the current tokens\n",
    "\n",
    "        return \" \".join(generated_text)\n",
    "    \n",
    "    def evaluate_model(self, train_sizes, validation_data):\n",
    "        train_perplexities = []\n",
    "        validation_perplexities = []\n",
    "        \n",
    "        for train_size in train_sizes:\n",
    "            train_data = self.data.sample(frac=train_size, random_state=42)\n",
    "            self.data = train_data\n",
    "            self.dataset_cleaning()\n",
    "            self.dataset_preprocess()\n",
    "            self.train()\n",
    "\n",
    "            train_corpus = \" \".join(train_data[\"Lyrics\"].dropna().tolist())\n",
    "            train_sentences = train_corpus.split(\"\\n\")\n",
    "            train_perplexity = np.mean([self.perplexity(sentence) for sentence in train_sentences if sentence.strip()])\n",
    "            train_perplexities.append(train_perplexity)\n",
    "\n",
    "            validation_corpus = \" \".join(validation_data[\"Lyrics\"].dropna().tolist())\n",
    "            validation_sentences = validation_corpus.split(\"\\n\")\n",
    "            validation_perplexity = np.mean([self.perplexity(sentence) for sentence in validation_sentences if sentence.strip()])\n",
    "            validation_perplexities.append(validation_perplexity)\n",
    "        \n",
    "        return train_perplexities, validation_perplexities\n",
    "\n",
    "# Function to calculate ROUGE score\n",
    "def calculate_rouge_score(generated_text, reference_texts):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = []\n",
    "\n",
    "    for reference_text in reference_texts:\n",
    "        score = scorer.score(reference_text, generated_text)\n",
    "        scores.append(score)\n",
    "\n",
    "    avg_scores = {}\n",
    "    for key in scores[0].keys():\n",
    "        avg_scores[key] = {\n",
    "            'precision': np.mean([score[key].precision for score in scores]),\n",
    "            'recall': np.mean([score[key].recall for score in scores]),\n",
    "            'fmeasure': np.mean([score[key].fmeasure for score in scores])\n",
    "        }\n",
    "    \n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Song Title  \\\n",
      "0            \\nShe Looks So Perfect Lyrics\\n   \n",
      "1                    \\nShape of You Lyrics\\n   \n",
      "2                         \\nPerfect Lyrics\\n   \n",
      "3                      \\nPhotograph Lyrics\\n   \n",
      "4               \\nThinking Out Loud Lyrics\\n   \n",
      "5                         \\nHappier Lyrics\\n   \n",
      "6                          \\nA Team Lyrics\\n   \n",
      "7                      \\nI See Fire Lyrics\\n   \n",
      "8             \\nSupermarket Flowers Lyrics\\n   \n",
      "9              \\nCastle on the Hill Lyrics\\n   \n",
      "10                   \\nGive Me Love Lyrics\\n   \n",
      "11                   \\nPerfect Duet Lyrics\\n   \n",
      "12                          \\nDon't Lyrics\\n   \n",
      "13               \\nAll Of The Stars Lyrics\\n   \n",
      "14               \\nPerfect Symphony Lyrics\\n   \n",
      "15                        \\nKiss Me Lyrics\\n   \n",
      "16                           \\nDive Lyrics\\n   \n",
      "17  \\nYou Need Me, I Don't Need You Lyrics\\n   \n",
      "18                     \\nLego House Lyrics\\n   \n",
      "19                   \\nTenerife Sea Lyrics\\n   \n",
      "\n",
      "                                               Lyrics  \n",
      "0    Hey, hey, hey, hey, Hey, hey, hey, hey, Hey, ...  \n",
      "1    The club isn't the best place to find a lover...  \n",
      "2    I found a love for me, Darling, just dive rig...  \n",
      "3    Loving can hurt, Loving can hurt sometimes, B...  \n",
      "4    When your legs don't work like they used to b...  \n",
      "5    Walking down 29th and park, I saw you in anot...  \n",
      "6    White lips, pale face, Breathing in snowflake...  \n",
      "7    Oh, misty eye of the mountain below, Keep car...  \n",
      "8    I took the supermarket flowers from the windo...  \n",
      "9    When I was six years old, I broke my leg, I w...  \n",
      "10   Give me love like her, 'Cause lately I've bee...  \n",
      "11   I found a love, For me, Oh darling, just dive...  \n",
      "12   La, la, la, la I met this girl late last year...  \n",
      "13   It's just another night, And I'm staring at t...  \n",
      "14   I found a love for me, Oh darling, just dive ...  \n",
      "15   Settle down with me, Cover me up, Cuddle me i...  \n",
      "16   Oh, maybe I came on too strong, Maybe I waite...  \n",
      "17   Now I'm in town, break it down, thinking of m...  \n",
      "18   I'm gonna pick up the pieces, And build a Leg...  \n",
      "19   You look so wonderful in your dress, I love y...  \n",
      "Sample probabilities: [(('<s>', '<s>', 'look'), 0.0018709073900841909), (('<s>', 'look', 'wonderful'), 0.001899335232668566), (('look', 'wonderful', 'dress'), 0.001899335232668566), (('wonderful', 'dress', 'love'), 0.0018975332068311196), (('dress', 'love', 'hair'), 0.001899335232668566), (('love', 'hair', 'like'), 0.001899335232668566), (('hair', 'like', 'way'), 0.0018975332068311196), (('like', 'way', 'falls'), 0.001899335232668566), (('way', 'falls', 'side'), 0.001899335232668566), (('falls', 'side', 'neck'), 0.001899335232668566)]\n",
      "Sample probabilities: [(('<s>', '<s>', 'look'), 0.004807692307692308), (('<s>', 'look', 'wonderful'), 0.004842615012106538), (('look', 'wonderful', 'dress'), 0.004842615012106538), (('wonderful', 'dress', 'love'), 0.004830917874396135), (('dress', 'love', 'hair'), 0.004842615012106538), (('love', 'hair', 'like'), 0.004842615012106538), (('hair', 'like', 'way'), 0.004830917874396135), (('like', 'way', 'falls'), 0.004842615012106538), (('way', 'falls', 'side'), 0.004842615012106538), (('falls', 'side', 'neck'), 0.004842615012106538)]\n",
      "Sample probabilities: [(('<s>', '<s>', 'oooh'), 0.023255813953488372), (('<s>', 'oooh', 'ooh'), 0.023529411764705882), (('oooh', 'ooh', 'ooh'), 0.05434782608695652), (('ooh', 'ooh', 'ooooh'), 0.056818181818181816), (('ooh', 'ooooh', 'oooh'), 0.056818181818181816), (('ooooh', 'oooh', 'oooh'), 0.056818181818181816), (('oooh', 'oooh', 'ooh'), 0.056818181818181816), (('oooh', 'ooh', 'oooh'), 0.043478260869565216), (('ooh', 'oooh', 'ooh'), 0.04597701149425287), (('oooh', 'ooh', 'poor'), 0.021739130434782608)]\n",
      "Sample probabilities: [(('<s>', '<s>', 'settle'), 0.041666666666666664), (('<s>', 'settle', 'cover'), 0.041666666666666664), (('settle', 'cover', 'cuddle'), 0.041666666666666664), (('cover', 'cuddle', 'lie'), 0.041666666666666664), (('cuddle', 'lie', 'yeah'), 0.041666666666666664), (('lie', 'yeah', 'hold'), 0.041666666666666664), (('yeah', 'hold', 'arms'), 0.041666666666666664), (('hold', 'arms', 'hearts'), 0.061224489795918366), (('arms', 'hearts', 'chest'), 0.061224489795918366), (('hearts', 'chest', 'lips'), 0.061224489795918366)]\n",
      "Sample probabilities: [(('<s>', '<s>', 'settle'), 0.041666666666666664), (('<s>', 'settle', 'cover'), 0.041666666666666664), (('settle', 'cover', 'cuddle'), 0.041666666666666664), (('cover', 'cuddle', 'lie'), 0.041666666666666664), (('cuddle', 'lie', 'yeah'), 0.041666666666666664), (('lie', 'yeah', 'hold'), 0.041666666666666664), (('yeah', 'hold', 'arms'), 0.041666666666666664), (('hold', 'arms', 'hearts'), 0.061224489795918366), (('arms', 'hearts', 'chest'), 0.061224489795918366), (('hearts', 'chest', 'lips'), 0.061224489795918366)]\n",
      "Sample probabilities: [(('<s>', '<s>', 'settle'), 0.041666666666666664), (('<s>', 'settle', 'cover'), 0.041666666666666664), (('settle', 'cover', 'cuddle'), 0.041666666666666664), (('cover', 'cuddle', 'lie'), 0.041666666666666664), (('cuddle', 'lie', 'yeah'), 0.041666666666666664), (('lie', 'yeah', 'hold'), 0.041666666666666664), (('yeah', 'hold', 'arms'), 0.041666666666666664), (('hold', 'arms', 'hearts'), 0.061224489795918366), (('arms', 'hearts', 'chest'), 0.061224489795918366), (('hearts', 'chest', 'lips'), 0.061224489795918366)]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset, train the model, and evaluate overfitting/underfitting\n",
    "lm = LanguageModel_NGram()\n",
    "lm.load_dataset(\"ed_sheeran_cleaned_songs.csv\")  # Load your dataset here\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, validation_data = train_test_split(lm.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define training sizes for the learning curve\n",
    "train_sizes = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "# Evaluate the model on different training sizes\n",
    "train_perplexities, validation_perplexities = lm.evaluate_model(train_sizes, validation_data)\n",
    "\n",
    "# Convert np.float64 to plain floats\n",
    "train_perplexities = [float(perplexity) for perplexity in train_perplexities]\n",
    "validation_perplexities = [float(perplexity) for perplexity in validation_perplexities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexities: [1035.5366081690615, 400.46830581303783, 79.7276169054956, 46.88670420679346, 46.88670420679346, 46.88670420679346]\n",
      "Validation Perplexities: [1042.1205884386327, 411.40948804715475, 84.7938465411506, 47.97665856617366, 47.97665856617366, 47.97665856617366]\n",
      "Generated Text: settle cover cuddle lie yeah hold arms hearts chest lips pressed neck im falling eyes dont know yet feeling ill forget im love kiss like wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna\n"
     ]
    }
   ],
   "source": [
    "# Organize and display the output\n",
    "print(\"Train Perplexities:\", train_perplexities)\n",
    "print(\"Validation Perplexities:\", validation_perplexities)\n",
    "\n",
    "# Generate new text\n",
    "generated_text = lm.generate_text(max_length=1000)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: 0.1383\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.1214\n"
     ]
    }
   ],
   "source": [
    "# Example reference texts for calculating ROUGE score\n",
    "reference = \" Things were all good yesterday, And then the devil took your memory, And if you fell to your death today, I hope that heaven is your resting place, I heard the doctors put your chest in pain, But then that could have been the medicine, And now you're lying in the bed again, Either way I'll cry with the rest of them My father told me, son, It's not his fault he doesn't know your face, You're not the only one, Although my grandma used to say, He used to sing Darlin' hold me in your arms, The way you did last night, And we'll die inside, For a little while here oh I could look into your eyes, Until the sun comes up, And we're wrapped in light and life and love, Put your open lips on mine, And slowly let them shut, For they're designed to be together oh, With your body next to mine, Our hearts will beat as one, And we set alight, We're afire love Love, love Things were all good yesterday, Then the devil took your breath away, Now we're left here in the pain, Black suit black tie standing in the rain, And now my family is one again, Stapled together with the strangers and a friend, Came to my mind I should paint it with a pen, Six years old I remember when My father told me, son, It's not his fault he doesn't know your face, You're not the only one, Although my grandma used to say, He used to sing Darlin' hold me in your arms, The way you did last night, And we'll die inside, For a little while here oh I could look into your eyes, Until the sun comes up, And we're wrapped in light and life and love, Put your open lips on mine, And slowly let them shut, For they're designed to be together oh, With your body next to mine, Our hearts will beat as one, And we're set alight, We're afire love Love, love See the love, the love, the love, the love, See the love, the love, the love, the love, See the love, the love, the love, the love My father and all of my family, Rise from their seats to sing hallelujah, And my mother and all of my family, Rise from their seats to say hallelujah, And my brother and all of my family, Rise from their seats to sing hallelujah, (To my brother and my sister, yeah, ah), My father and all of my family, Rise from their seats to sing, Hallelujah To the love, the love, the love, the love, To the love, the love, the love, the love, To the love, the love, the love, the love\"\n",
    "generated = \"settle cover cuddle lie yeah hold arms hearts chest lips pressed neck im falling eyes dont know yet feeling ill forget im love kiss like wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna loved wanna\"\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(reference, generated)\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "print(f\"ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
